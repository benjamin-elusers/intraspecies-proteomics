```{r load-data-proteomics, echo=F}
chap_cur=03
#load(here('output',sprintf('%02d-proteomics.rdata',chap_cur-1)))
```

# Data processing

Before normalizing intensities, we first need to discard some hits because they
are not suitable for analysis:

 1. contaminants (*identifier starts with CON*)
 2. reversed sequences (*identifier starts with REV*)
 3. multiple hits (*peptides matching several proteins*)
 4. not enough unique peptides (*low confidence identification (< 2 peptides)*)

```{r filter-data}
# Filter hits
ms1=filter_hits(ms0)
```

## Processing quantified intensities

Following the aim of this experiment, we wish to compare the variation of protein 
expression between strains. 

Once all samples have been filtered to exclude outliers hits, we can process the 
quantified intensities.

To avoid introducing potential biases, we will monitor the intensities over 4
stages of processing:

1. raw peptide intensities `int_raw`
2. lfq intensities `int_lfq`
3. normalization across strains (median equalization) `int_norm`
4. BPCA imputation on missing values (Bayesian PCA) `int_bpca`

Initially, samples may have large difference between raw peptide intensities 
for numerous technical and experimental reasons including but not limited to 
protein digestion (length), sample preparation, sample processing...

Typically, we use Label-Free Quantitation (LFQ) intensities [@maxlfq] which eliminates
error between samples by reconstructing the abundance profile based on all available
individual protein ratios.

```{r process-intensities}
# Process intensities
# "majority_protein_i_ds"
int_raw= get_intensities(ms1, regex_int='^intensity_', col_id=keys_cols[1], remove_prefix=T)
int_lfq= get_intensities(ms1, regex_int='^lfq_intensity_', col_id=keys_cols[1], remove_prefix=T)

longint_raw = make_intensities_long(ms1, regex_int='^intensity_',col_id=keys_cols[1], remove_prefix=T)
longint_lfq = make_intensities_long(ms1, regex_int='^lfq_intensity_', col_id=keys_cols[1], remove_prefix=T)

```

Then, we can normalize the LFQ intensities across samples using various methods
(see @Normalyzer package for more details). A simple normalization technique consists
in equalizing the median across samples. 

**Normalization method = Equalizing medians**

$$ norm.int_{sample} = log2(raw.int_{sample}) - log2(median_{sample}) $$

Here, we averaged the intensities over replicates to compare strains and transform 
intensities to log2. Then we equalize their medians by subtracting the samples 
median and adding the absolute minimum value across all samples. The hits that
contain missing values for either strains or replicates are discarded then.

```{r normalize-intensities}
int_md_norm = center_intensities(int_lfq, center='median', tolog2=T) %>% as.data.frame()
int_norm = int_md_norm
int_norm_nona = int_norm %>% drop_na()
int_norm_ids = rownames(int_norm)

# TRYING DIFFERENT NORMALIZATIONS SCHEME (NOT READY)
INT_NORM = normalize_intensities(int = int_lfq, design = df.group)
NORM_METHODS = names(INT_NORM@normalizations)
df_all_norm <- INT_NORM@normalizations %>% 
               map2_df(.y = NORM_METHODS,~mutate(id=rownames(.x),as_tibble(.x),method=.y))

TAB_NORM = kbl(NORM_METHODS,row.names = T,col.names = 'Normalizations:',position = 'left') %>% 
            kable_paper("striped", full_width = F) %>% 
            kable_styling(position='left')  

```


Finally, we can try to rescue the hits with missing values by doing imputation with
a Bayesian PCA method (see @MSCoreUtils).

```{r impute-na}
library(MsCoreUtils)
int_bpca = MsCoreUtils::impute_bpca(int_norm) %>% as.data.frame()
df_int_bpca = int_bpca %>% as_tibble() %>% add_column(uniprot = int_norm_ids)
df_int_norm = int_norm %>% add_column(uniprot = int_norm_ids)
```

We also convert expression matrix to long tabular format with intensities across
samples and/or strains for further quality controls and plots.

```{r int-strains}
long_int_norm = pivot_longer(df_int_bpca  , cols=-uniprot, values_to='int2use',
                             names_to = c('strain','bio','tech','day'),
                             names_pattern = "([^_]+)_([^_]+)_([^_]+)_([^_]+)") %>%
                group_by(strain,uniprot) %>% mutate(na_rep = sum.na(int2use))

# Intensities across strains  (default is average)
int_by_strain = pivot_wider(long_int_norm , id_cols=uniprot,
              names_from = 'strain', names_glue = "{strain}",
              values_from = "int2use", values_fn=mean_)

na_by_strain = pivot_wider(long_int_norm, id_cols=uniprot,
              names_from = c('strain'), names_glue = "na_rep_{strain}",
              values_from = "int2use", values_fn=sum.na)

df_strains= left_join(int_by_strain,na_by_strain) %>% 
            rowwise %>%
            mutate( na_strains = sum.na(c_across(cols = starts_with('lfq_int'))) )

int_all = int_bpca 

# Removing hits with missing values for more than one strain (using average intensities over replicates)
ms2= df_strains %>% filter(na_strains < 1)
int_filt_strains = ms2 %>% dplyr::select(-starts_with('na')) %>% column_to_rownames('uniprot') %>% as.data.frame()
```

```{r 02-save-data, echo=F}
saveRDS(int_raw,here::here('output','int_raw.rds'))
saveRDS(int_lfq,here::here('output','int_lfq.rds'))
saveRDS(int_md_norm,here::here('output','log2-int_lfq-norm_median.rds'))
saveRDS(int_bpca,here::here('output','log2-int_lfq-na_imputed-norm_median.rds'))
saveRDS(int_by_strain,here::here('output','log2-int_lfq_bystrains-na_imputed-norm_median.rds'))
save.image(here('output',sprintf("%02d-processed-data.rdata",chap_cur)))
```